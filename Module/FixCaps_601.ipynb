{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "uuid": "5cde640c-432f-4f8a-9f00-017bf8c65e34"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys, os\n",
    "import json\n",
    "import torch.nn as nn  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import prettytable\n",
    "import time\n",
    "sys.setrecursionlimit(18000)\n",
    "from thop.profile import profile\n",
    "\n",
    "from PIL import Image\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchsummary import summary\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from images_show import ImageShow\n",
    "from model601 import FixCapsNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "uuid": "a391afe8-e37b-4ef6-ab2e-c0bdbb7849ee"
   },
   "outputs": [],
   "source": [
    "# Settings.\n",
    "sys.path.append(os.pardir)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "img_title = \"HAM10000\"#\"ISIC2019\"#\"skin_lesion\"#\n",
    "best_acc= 0.\n",
    "\n",
    "write = SummaryWriter()\n",
    "#defined \n",
    "try:\n",
    "    print(len(train_acc_list))\n",
    "except NameError:\n",
    "    train_loss_list = []\n",
    "    train_acc_list = []\n",
    "    test_loss_list = []\n",
    "    test_acc_list = []\n",
    "    test_auc_list = []\n",
    "    val_loss_list = []\n",
    "    val_acc_list = []\n",
    "#activate ImageShow\n",
    "show = ImageShow(train_loss_list = train_loss_list,\n",
    "                 train_acc_list = train_acc_list,\n",
    "                test_loss_list = test_loss_list,\n",
    "                test_acc_list = test_acc_list,\n",
    "                test_auc_list = test_auc_list,\n",
    "                val_loss_list = val_loss_list,\n",
    "                val_acc_list = val_acc_list,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_transform = {\n",
    "    \"train\": transforms.Compose([transforms.RandomResizedCrop((299, 299)),\n",
    "                                 transforms.RandomVerticalFlip(),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]),\n",
    "    \"test\": transforms.Compose([transforms.Resize((308, 308)),\n",
    "                               transforms.CenterCrop((299, 299)),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                              ])\n",
    "    }\n",
    "train_doc = \"train530_8k\"\n",
    "test_doc = \"test530\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_root = os.path.abspath(os.path.join(os.getcwd(),\"..\"))  # get data root path\n",
    "image_path = os.path.join(data_root, \"datasets\",\"skin_lesion_types\",\"HAM\")#\n",
    "assert os.path.exists(image_path), \"{} path does not exist.\".format(image_path)\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=os.path.join(image_path,train_doc),#\n",
    "                                     transform=data_transform[\"train\"])\n",
    "test_dataset = datasets.ImageFolder(root=os.path.join(image_path,test_doc),\n",
    "                                        transform=data_transform[\"test\"])\n",
    "\n",
    "train_num = len(train_dataset)\n",
    "test_num = len(test_dataset)\n",
    "\n",
    "img_sample = Image.open(test_dataset.imgs[1][0])\n",
    "n_channels = np.array(img_sample).shape[2]#(H,W,C)\n",
    "\n",
    "data_list = train_dataset.class_to_idx\n",
    "cla_dict = dict((val, key) for key, val in data_list.items())\n",
    "n_classes  = len(data_list)\n",
    "print(f'Using {n_classes } classes.')\n",
    "# write dict into json file\n",
    "json_str = json.dumps(cla_dict, indent=4)\n",
    "with open(f'{img_title}.json', 'w') as json_file:#class_indices\n",
    "    json_file.write(json_str)\n",
    "    \n",
    "BatchSize = 188#112#168\n",
    "T_size = 31#32#64\n",
    "nw = min([os.cpu_count(), BatchSize if BatchSize > 1 else 0, 6]) \n",
    "print(f'Using {nw} dataloader workers every process.')\n",
    "pin_memory = True\n",
    "train_loader = DataLoader(train_dataset,batch_size=BatchSize,\n",
    "                                           pin_memory=pin_memory,\n",
    "                                           shuffle=True,num_workers=nw)\n",
    "test_loader = DataLoader(test_dataset,batch_size=T_size,\n",
    "                                          pin_memory=pin_memory,\n",
    "                                          shuffle=False,num_workers=nw)\n",
    "\n",
    "print(\"using {} images for training, {} images for validation.\".format(train_num, test_num))\n",
    "# using 51699(aug_images = 8000) images for training, 828 images for validation. 96.13% 3090\n",
    "# using 29263(aug_images = 4000) images for training, 828 images for validation. 95.05%\n",
    "# using 17722(aug_images = 2000) images for training, 828 images for validation. 94.57%\n",
    "# using 54292 images for training, 828 images for validation. 95.53%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create capsule network.\n",
    "conv_outputs = 128 #Feature_map\n",
    "num_primary_units = 8\n",
    "primary_unit_size = 16 * 6 * 6  # fixme get from conv2d\n",
    "output_unit_size = 16\n",
    "img_size = 299\n",
    "mode='DS'\n",
    "network = FixCapsNet(conv_inputs=n_channels,\n",
    "                     conv_outputs=conv_outputs,\n",
    "                     primary_units=num_primary_units,\n",
    "                     primary_unit_size=primary_unit_size,\n",
    "                     num_classes=n_classes,\n",
    "                     output_unit_size=16,\n",
    "                     init_weights=True,\n",
    "                     mode=mode)\n",
    "network = network.to(device)\n",
    "summary(network,(n_channels,img_size,img_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"%s | %s | %s\" % (\"Model\", \"Params(M)\", \"FLOPs(G)\"))\n",
    "print(\"---|---|---\")\n",
    "name = \"FixCaps\"\n",
    "dsize = (1, 3, 299, 299)\n",
    "inputs = torch.randn(dsize).to(device)\n",
    "total_ops, total_params = profile(network, (inputs,), verbose=False)\n",
    "print(\n",
    "    \"%s | %.2f | %.2f\" % (name, total_params / (1000 ** 2), total_ops / (1000 ** 3))\n",
    "    )\n",
    "#FLOPs(G)--> 0.07(0.08)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "network.Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Converts batches of class indices to classes of one-hot vectors.\n",
    "def to_one_hot(x, length):\n",
    "    batch_size = x.size(0)\n",
    "    x_one_hot = torch.zeros(batch_size, length)\n",
    "    for i in range(batch_size):\n",
    "        x_one_hot[i, x[i]] = 1.0\n",
    "    return x_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(evl_result):\n",
    " \n",
    "    plt.style.use(\"seaborn\")#classic\n",
    "    plt.yticks(range(n_classes),cla_dict.values())\n",
    "    plt.xticks(range(n_classes),cla_dict.values())\n",
    "\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.imshow(evl_result,cmap=plt.cm.Greens)\n",
    "    for i in range(len(evl_result)):\n",
    "        for j in range(len(evl_result[i])):\n",
    "            plt.text(j,i,int(evl_result[i][j]))\n",
    "\n",
    "    plt.colorbar()\n",
    "    plt.savefig(f\"./tmp/{img_title}/{suf}/Confusion Matrix.png\",dpi=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def self_scores(opt=\"scores\"):\n",
    "    result_table = prettytable.PrettyTable()\n",
    "    result_table.field_names = ['Type','Precision', 'Recall', 'F1','Accuracy']    \n",
    "    accuracy = float(torch.sum(evl_result.diagonal())/torch.sum(evl_result))  \n",
    "    for i in range(n_classes):\n",
    "        pre = float(evl_result[i][i] / torch.sum(evl_result,0)[i])#列和\n",
    "        recall = float(evl_result[i][i] / torch.sum(evl_result,1)[i])#行和torch.sum(result_table[i])\n",
    "        F1 = pre * recall * 2 / (pre + recall + 1e-8)\n",
    "        result_table.add_row([cla_dict[i], round(pre, 4), round(recall, 3), round(F1, 3),\" \"])\n",
    "\n",
    "    result_table.add_row([\"Total:\", \" \", \" \", \" \",round(accuracy,4)])\n",
    "    print(result_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "uuid": "86f92f63-16e2-4ca6-b5da-d848546a391e"
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    network.train()\n",
    "    running_loss,r_pre = 0., 0.\n",
    "    print_step = len(train_loader)//2\n",
    "    print(f'Train Epoch:[{epoch}]{img_title} ==> Training ...')\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(tqdm(train_loader)):        \n",
    "\n",
    "        batch_idx += 1\n",
    "        target_indices = target\n",
    "        target_one_hot = to_one_hot(target, length=n_classes)\n",
    "        data, target = Variable(data).to(device), Variable(target_one_hot).to(device)\n",
    "        # data, target =data.to(device), target_one_hot.to(device)\n",
    "\n",
    "        \n",
    "        # with torch.cuda.amp.autocast():\n",
    "        output = network(data)\n",
    "        loss = network.loss(output, target, size_average=True)\n",
    "        \n",
    "        loss.backward()\n",
    "        # scaler.scale(loss).backward()\n",
    "        # scaler.step(optimizer)\n",
    "        # scaler.update()\n",
    "        \n",
    "        if mode == 'DS2':\n",
    "            if batch_idx % 2 == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                # scheduler.step()#OneCycleLR\n",
    "        else:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            # scheduler.step()#OneCycleLR\n",
    "            \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        v_mag = torch.sqrt(torch.sum(output**2, dim=2, keepdim=True)) \n",
    "        pred = v_mag.data.max(1, keepdim=True)[1].cpu().squeeze()\n",
    "        r_pre += pred.eq(target_indices.view_as(pred)).squeeze().sum()\n",
    "        \n",
    "        if batch_idx % print_step == 0:\n",
    "            print(\"[{}/{}] Loss{:.5f},ACC:{:.5f}%\".format(batch_idx,len(train_loader),loss,\n",
    "                                                         r_pre/(batch_idx*BatchSize)))\n",
    "           \n",
    "        \n",
    "    epoch_acc = r_pre / train_num\n",
    "    epoch_loss = running_loss / len(train_loader)  \n",
    "    train_loss_list.append(epoch_loss)\n",
    "    train_acc_list.append(epoch_acc) \n",
    "    \n",
    "    scheduler.step()#CosineAnnealingLR\n",
    "    print(f'Train Epoch: [{epoch}] Loss: {epoch_loss},Train_acc:{round(float(epoch_acc),3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "uuid": "f010e769-677f-416b-8ce8-9f4186dce054"
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    network.eval()\n",
    "    global best_acc,evl_result,evl_tmp_result,net_parameters\n",
    "    cor_loss,correct,Auc, Acc= 0, 0, 0, 0\n",
    "    evl_tmp_result = torch.zeros(n_classes,n_classes)\n",
    "    steps_num = len(test_loader)\n",
    "    tmp_size = T_size\n",
    "    print(f'\\033[35m{img_title} ==> testing ...\\033[0m')\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(tqdm(test_loader)):\n",
    "            batch_idx +=1\n",
    "            target_indices = target#torch.Size([batch, 7])  \n",
    "            target_one_hot = to_one_hot(target, length=n_classes)            \n",
    "            data, target = Variable(data).to(device), Variable(target_one_hot).to(device)\n",
    "            # data, target =data.to(device), target_one_hot.to(device)\n",
    "\n",
    "            output= network(data)#torch.Size([batch_size, 7, 16, 1])         \n",
    "            v_mag = torch.sqrt(torch.sum(output**2, dim=2, keepdim=True))\n",
    "            pred = v_mag.data.max(1, keepdim=True)[1].cpu()#[9, 2, 1, 1, 6,..., 1, 4, 6, 5, 7,]\n",
    "            \n",
    "            if batch_idx % steps_num == 0 and test_num % T_size != 0:\n",
    "                tmp_size = test_num % T_size\n",
    "                          \n",
    "            for i in range(tmp_size):\n",
    "                pred_y = pred.numpy()\n",
    "                evl_tmp_result[target_indices[i]][pred_y[i]] +=1 \n",
    "\n",
    "    diag_sum = torch.sum(evl_tmp_result.diagonal())\n",
    "    all_sum = torch.sum(evl_tmp_result) \n",
    "    accuracy = 100. * float(torch.div(diag_sum,all_sum)) \n",
    "    test_acc_list.append(accuracy)\n",
    "    write.add_scalar('acc/test', accuracy, epoch)\n",
    "    print(f\"Test_Acc:\\033[1;32m{round(float(accuracy),3)}%\\033[0m\")\n",
    "\n",
    "    if accuracy >= best_acc:\n",
    "        best_acc = accuracy\n",
    "        evl_result = evl_tmp_result.clone()#copy.deepcopy(input)\n",
    "        torch.save(network.state_dict(), save_PATH)\n",
    "        torch.save(evl_result, f'./tmp/{img_title}/{suf}/best_evl_result.pt')\n",
    "        \n",
    "    print(f\"Best_ACC：\\033[1;32m[{round(float(best_acc),3)}%]\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create store\n",
    "try:\n",
    "    print(f\"suf:{suf}\")\n",
    "except NameError:\n",
    "    suf = time.strftime(\"%m%d_%H%M%S\", time.localtime())\n",
    "    print(f\"suf:{suf}\")   \n",
    "if os.path.exists(f'./tmp/{img_title}/{suf}'):\n",
    "    print (f'Store: \"./tmp/{img_title}/{suf}\"')\n",
    "else:\n",
    "    !mkdir -p ./tmp/{img_title}/{suf} \n",
    "save_PATH = f'./tmp/{img_title}/{suf}/new_best_{img_title}_{suf}.pth'\n",
    "print(save_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [],
    "uuid": "edaf3774-de76-44a5-aabd-fb52532ea36e"
   },
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "# network.load_state_dict(torch.load(\"./best_DS_9613.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.123\n",
    "def_betas=(0.9, 0.999)\n",
    "optimizer = optim.Adam(network.parameters(), lr=learning_rate)\n",
    "# optimizer = optim.AdamW(network.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, 5, eta_min=1e-8, last_epoch=-1)\n",
    "\n",
    "#base_momentum=0.85, max_momentum=0.95, div_factor=25.0, final_div_factor=10000.0,\n",
    "# scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate,\n",
    "#                         steps_per_epoch=len(train_loader),\n",
    "#                         epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [],
    "uuid": "edaf3774-de76-44a5-aabd-fb52532ea36e"
   },
   "outputs": [],
   "source": [
    "for epoch in range(1, num_epochs + 1): \n",
    "    train(epoch)\n",
    "    test()\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show.conclusion(img_title=img_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.clf()#clear photo\n",
    "show.train(write=True,custom_path='./tmp',img_title=img_title,suf=suf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show.train(opt='Acc',write=True,custom_path='./tmp',img_title=img_title,suf=suf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show.test(write=True,custom_path='./tmp',img_title=img_title,suf=suf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "network.load_state_dict(torch.load(save_PATH))\n",
    "# model_weight_path = \"Best_DS_9613.pth\"\n",
    "# pre_weights = torch.load(model_weight_path, map_location=device)\n",
    "# pre_dict = {k: v for k, v in pre_weights.items() if network.state_dict()[k].numel() == v.numel()}\n",
    "# missing_keys, unexpected_keys = network.load_state_dict(pre_dict, strict=False)\n",
    "for i in range(5):   \n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "self_scores()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "+--------+-----------+--------+-------+----------+\n",
    "|  Type  | Precision | Recall |   F1  | Accuracy |\n",
    "+--------+-----------+--------+-------+----------+\n",
    "| akiec  |    0.88   | 0.957  | 0.917 |          |\n",
    "|  bcc   |   0.9565  | 0.846  | 0.898 |          |\n",
    "|  bkl   |   0.8676  | 0.894  | 0.881 |          |\n",
    "|   df   |   0.5714  | 0.667  | 0.615 |          |\n",
    "|  mel   |   0.9394  | 0.912  | 0.925 |          |\n",
    "|   nv   |   0.9835  | 0.986  | 0.985 |          |\n",
    "|  vasc  |    1.0    |  0.7   | 0.824 |          |\n",
    "| Total: |           |        |       |  0.965   |\n",
    "+--------+-----------+--------+-------+----------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "confusion_matrix(evl_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #save\n",
    "s0 = np.array(train_acc_list)\n",
    "np.save(f'./tmp/{img_title}/{suf}/{img_title}_train_acc_{suf}.npy', s0)\n",
    "s1 = np.array(train_loss_list)\n",
    "np.save(f'./tmp/{img_title}/{suf}/{img_title}_train_loss_{suf}.npy', s1)\n",
    "s2 = np.array(test_acc_list)\n",
    "np.save(f'./tmp/{img_title}/{suf}/{img_title}_test_acc_{suf}.npy', s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_transform = {\n",
    "    \"train\": transforms.Compose([transforms.RandomResizedCrop((299, 299)),\n",
    "                                 transforms.RandomVerticalFlip(),\n",
    "                                 #transforms.RandomHorizontalFlip(),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]),\n",
    "    \"test\": transforms.Compose([transforms.Resize((312, 312)),\n",
    "                               transforms.CenterCrop((299, 299)),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                              ])\n",
    "    }\n",
    "\n",
    "data_root = os.path.abspath(os.path.join(os.getcwd(),\"..\"))  # get data root path\n",
    "image_path = os.path.join(data_root, \"datasets\",\"skin_lesion_types\",\"HAM\")#\n",
    "assert os.path.exists(image_path), \"{} path does not exist.\".format(image_path)\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=os.path.join(image_path,train_doc),#\n",
    "                                     transform=data_transform[\"train\"])\n",
    "test_dataset = datasets.ImageFolder(root=os.path.join(image_path,test_doc),\n",
    "                                        transform=data_transform[\"test\"])\n",
    "\n",
    "train_num = len(train_dataset)\n",
    "test_num = len(test_dataset)\n",
    "\n",
    "img_sample = Image.open(test_dataset.imgs[1][0])\n",
    "n_channels = np.array(img_sample).shape[2]#(H,W,C)\n",
    "\n",
    "data_list = train_dataset.class_to_idx\n",
    "cla_dict = dict((val, key) for key, val in data_list.items())\n",
    "n_classes  = len(data_list)\n",
    "print(f'Using {n_classes } classes.')\n",
    "# write dict into json file\n",
    "json_str = json.dumps(cla_dict, indent=4)\n",
    "with open(f'{img_title}.json', 'w') as json_file:#class_indices\n",
    "    json_file.write(json_str)\n",
    "    \n",
    "BatchSize = 112#144#188\n",
    "T_size = 31\n",
    "nw = min([os.cpu_count(), BatchSize if BatchSize > 1 else 0, 6]) \n",
    "print(f'Using {nw} dataloader workers every process.')\n",
    "pin_memory = True\n",
    "train_loader = DataLoader(train_dataset,batch_size=BatchSize,\n",
    "                                           pin_memory=pin_memory,\n",
    "                                           shuffle=True,num_workers=nw)\n",
    "test_loader = DataLoader(test_dataset,batch_size=T_size,\n",
    "                                          pin_memory=pin_memory,\n",
    "                                          shuffle=False,num_workers=nw)\n",
    "\n",
    "print(\"using {} images for training, {} images for validation.\".format(train_num, test_num))\n",
    "# using 51699(Aug) images for training, 828 images for validation. IRv2-SA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "network.load_state_dict(torch.load(save_PATH))\n",
    "for i in range(5):   \n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "self_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# show.test(write=True,custom_path='./tmp',img_title=img_title,suf=suf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #save\n",
    "s0 = np.array(train_acc_list)\n",
    "np.save(f'./tmp/{img_title}/{suf}/{img_title}_train_acc_{suf}.npy', s0)\n",
    "s1 = np.array(train_loss_list)\n",
    "np.save(f'./tmp/{img_title}/{suf}/{img_title}_train_loss_{suf}.npy', s1)\n",
    "s2 = np.array(test_acc_list)\n",
    "np.save(f'./tmp/{img_title}/{suf}/{img_title}_test_acc_{suf}.npy', s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "confusion_matrix(evl_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.memory.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 读取\n",
    "# sb = '0515_173222'\n",
    "# s1 = np.load(f'./tmp/{img_title}/{sb}/{img_title}_train_acc_{sb}.npy')\n",
    "# train_acc_list = s1.tolist()\n",
    "# s2 = np.load(f'./tmp/{img_title}/{sb}/{img_title}_train_loss_{sb}.npy')\n",
    "# train_loss_list = s2.tolist()\n",
    "# s3 = np.load(f'./tmp/{img_title}/{sb}/{img_title}_test_acc_{sb}.npy')\n",
    "# test_acc_list = s3.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dummy_input= torch.rand(128, 3, 299, 299).to(device)\n",
    "# with SummaryWriter(comment='FixCaps') as w:\n",
    "#     w.add_graph(network, (dummy_input,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# from prefetch_generator import BackgroundGenerator\n",
    "\n",
    "# class DataLoaderX(DataLoader):\n",
    "\n",
    "#     def __iter__(self):\n",
    "#         return BackgroundGenerator(super().__iter__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1.8",
   "language": "python",
   "name": "woodman"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
